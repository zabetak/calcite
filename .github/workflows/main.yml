# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to you under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# The default workflow for GitHub Actions that is used for continuous
# integration. A configuration file that is used to control when, where,
# and how different CI jobs are executed.
# For more information on how to modify this file check the following link:
# https://help.github.com/en/actions/automating-your-workflow-with-github-actions

name: CI

on:
  push:
    paths-ignore:
      - 'site/**'
    branches:
      - '*'

# Throw OutOfMemoryError in case less than 35% is free after full GC
# This avoids never-ending GC trashing if memory gets too low in case of a memory leak
env:
  _JAVA_OPTIONS: '-XX:GCTimeLimit=90 -XX:GCHeapFreeLimit=35'

jobs:
  linux-druid:
    name: 'Linux (JDK 8) Druid Tests'
    runs-on: ubuntu-latest
    steps:
    - name: 'Set up JDK 8'
      uses: actions/setup-java@v2
      with:
        java-version: 8
        distribution: 'zulu'
    - name: 'Checkout Druid dataset'
      uses: actions/checkout@v3
      with:
        repository: zabetak/calcite-druid-dataset
        fetch-depth: 1
        path: druid-dataset
    - name: 'Start Druid containers'
      working-directory: ./druid-dataset
      run: |
        chmod -R 777 storage
        docker-compose up -d
    - name: 'Wait Druid nodes to startup'
      run: |
        until docker logs coordinator | grep "Successfully started lifecycle \[module\]"; do sleep 1s; done
        until docker logs router | grep "Successfully started lifecycle \[module\]"; do sleep 1s; done
        until docker logs historical | grep "Successfully started lifecycle \[module\]"; do sleep 1s; done
        until docker logs middlemanager | grep "Successfully started lifecycle \[module\]"; do sleep 1s; done
        until docker logs broker | grep "Successfully started lifecycle \[module\]"; do sleep 1s; done
    - name: 'Index Foodmart/Wikipedia datasets'
      working-directory: ./druid-dataset
      run: ./index.sh 30s
    - name: 'Setup async-profiler' 
      run: |
        wget -c https://github.com/jvm-profiling-tools/async-profiler/releases/download/v2.9/async-profiler-2.9-linux-x64.tar.gz 
        tar -xvf async-profiler-2.9-linux-x64.tar.gz
        sudo sysctl kernel.perf_event_paranoid=1
        mkdir async-profiler-results
    - uses: actions/checkout@v3
      with:
        fetch-depth: 1
        path: calcite
    - uses: burrunan/gradle-cache-action@v1
      name: 'Run Druid tests'
      timeout-minutes: 10
      continue-on-error: true
      env:
        S3_BUILD_CACHE_ACCESS_KEY_ID: ${{ secrets.S3_BUILD_CACHE_ACCESS_KEY_ID }}
        S3_BUILD_CACHE_SECRET_KEY: ${{ secrets.S3_BUILD_CACHE_SECRET_KEY }}
      with:
        build-root-directory: ./calcite
        job-id: Druid8
        remote-build-cache-proxy-enabled: false
        arguments: --scan --no-parallel --no-daemon :druid:test -Dcalcite.test.druid=true
    - uses: actions/upload-artifact@v3
      with:
        path: ${{ github.workspace }}/async-profiler-results/*